{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a4a834",
   "metadata": {},
   "source": [
    "### This is Natural Language Processing Project in which Natural Language Toolkit is used and base on Extraction Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cdced",
   "metadata": {},
   "source": [
    "## Preparing the data. Here data is scraping from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "50b24301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as BeautifulSoup  #use to fetcha the articles text\n",
    "import urllib.request        #use for connectiong to the page and retrieving the HTML\n",
    "\n",
    "# Fetching the content from the URL\n",
    "fetched_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Brahma_Kumaris')\n",
    "\n",
    "article_read = fetched_data.read()\n",
    "\n",
    "# Parsing the URL content and storing in a variable\n",
    "article_parsed = BeautifulSoup.BeautifulSoup(article_read,'html.parser')\n",
    "\n",
    "# Returning <p> tags\n",
    "paragraphs = article_parsed.find_all('p')\n",
    "\n",
    "article_content = ''\n",
    "\n",
    "# Looping through the paragraphs and adding them to the variable\n",
    "for p in paragraphs:  \n",
    "    article_content += p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad37a0",
   "metadata": {},
   "source": [
    "## Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f9092eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "def _create_dictionary_table(text_string) -> dict:\n",
    "   \n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    words = word_tokenize(text_string)\n",
    "    \n",
    "    # Reducing words to their root form\n",
    "    stem = PorterStemmer()\n",
    "    \n",
    "    # Creating dictionary for the word frequency table\n",
    "    frequency_table = dict()\n",
    "    for wd in words:\n",
    "        wd = stem.stem(wd)\n",
    "        if wd in stop_words:\n",
    "            continue\n",
    "        if wd in frequency_table:\n",
    "            frequency_table[wd] += 1\n",
    "        else:\n",
    "            frequency_table[wd] = 1\n",
    "\n",
    "    return frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "baf4505e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and', 'doesn', 'by', 'hasn', 'before', \"should've\", 'nor', 'for', 'yourself', \"you'll\", 'these', 'their', 'your', \"wouldn't\", 'o', 'aren', 'but', 'you', 'if', \"didn't\", 'too', \"needn't\", 'am', \"shouldn't\", 'ours', 'will', 'haven', 'it', \"it's\", 'as', 'm', 'once', 's', 'now', 'couldn', 'yourselves', 'each', 'into', \"haven't\", 'him', 'do', 'not', 'did', 'other', 'yours', 'its', \"you'd\", 'are', 'while', 'her', \"hadn't\", 'the', 'why', 'she', 'has', 'against', 'i', 'through', 'then', 'some', 'itself', \"that'll\", 'above', 'all', \"she's\", 'have', 'further', 'herself', 'whom', 'same', 'ourselves', 'myself', 'where', 'needn', 'very', 'about', 'was', 'at', \"don't\", 'can', 'didn', 'won', 'themselves', 'during', 'to', 'don', 'himself', 'ma', 'here', 'under', 'hadn', 'should', 'y', 'be', 'weren', \"couldn't\", 'again', 'who', 'having', 'own', 'isn', 'below', 'such', \"isn't\", 'in', 'after', 'that', 're', \"won't\", 'those', 'until', 'mightn', \"you're\", 'our', 'any', 'both', \"aren't\", 'mustn', 'does', \"wasn't\", 'between', \"mustn't\", 'we', 'so', 'than', 'shouldn', \"doesn't\", 'my', 'because', 't', 'there', 'wasn', 'd', 'out', 'ain', \"shan't\", \"you've\", \"mightn't\", 'when', 'on', 'over', 'few', 'theirs', 'how', 'being', 'more', 'hers', 'an', 'of', 'he', 'me', 'down', 'this', 'from', 'is', 'just', \"hasn't\", 'they', 'll', 'doing', 'or', 'wouldn', 'a', 'them', \"weren't\", 'most', 'only', 'his', 'were', 've', 'with', 'been', 'which', 'no', 'had', 'off', 'shan', 'what', 'up'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a7e91ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_table=_create_dictionary_table(article_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9011c64a",
   "metadata": {},
   "source": [
    "## Tokenizing the article into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "05834d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "sentences = sent_tokenize(article_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee475cea",
   "metadata": {},
   "source": [
    "## Finding the weighted frequencies of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9897ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_sentence_scores(sentences, frequency_table) -> dict:   \n",
    "\n",
    "    # Algorithm for scoring a sentence by its words\n",
    "    sentence_weight = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_wordcount = (len(word_tokenize(sentence)))\n",
    "        sentence_wordcount_without_stop_words = 0\n",
    "        for word_weight in frequency_table:\n",
    "            if word_weight in sentence.lower():\n",
    "                sentence_wordcount_without_stop_words += 1\n",
    "                if sentence[:7] in sentence_weight:\n",
    "                    sentence_weight[sentence[:7]] += frequency_table[word_weight]\n",
    "                else:\n",
    "                    sentence_weight[sentence[:7]] = frequency_table[word_weight]\n",
    "\n",
    "        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] /        sentence_wordcount_without_stop_words\n",
    "      \n",
    "    return sentence_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4c89d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores=_calculate_sentence_scores(sentences, frequency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba544a4",
   "metadata": {},
   "source": [
    "## Calculating the threshold of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "80674654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_average_score(sentence_weight) -> int:\n",
    "   \n",
    "    # Calculating the average score for the sentences\n",
    "    sum_values = 0\n",
    "    for entry in sentence_weight:\n",
    "        sum_values += sentence_weight[entry]\n",
    "\n",
    "    # Getting sentence average value from source text\n",
    "    average_score = (sum_values / len(sentence_weight))\n",
    "\n",
    "    return average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "848ab3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = _calculate_average_score(sentence_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f5793",
   "metadata": {},
   "source": [
    "## Function to generate a summary from article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6769561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_article_summary(sentences, sentence_weight, threshold):\n",
    "    sentence_counter = 0\n",
    "    article_summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\n",
    "            article_summary += \" \" + sentence\n",
    "            sentence_counter += 1\n",
    "\n",
    "    return article_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6f307a46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [45] A flagship slogan for the BKs has been When we change, the world changes.\n"
     ]
    }
   ],
   "source": [
    "#producing the summary\n",
    "summary_results = _get_article_summary(sentences, sentence_scores, 1.9* threshold)\n",
    "print(summary_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705a07f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
